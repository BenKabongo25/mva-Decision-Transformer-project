{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../mini_rl_lib/\")\n",
    "from mdp import *\n",
    "from policies import *\n",
    "from wrappers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_probabilities = np.array([\n",
    "    [[0.50, 0.00, 0.50],\n",
    "     [0.70, 0.10, 0.20],\n",
    "     [0.40, 0.00, 0.60]],\n",
    "    [[0.00, 0.00, 1.00],\n",
    "     [0.00, 0.95, 0.05],\n",
    "     [0.30, 0.30, 0.40]],\n",
    "])\n",
    "\n",
    "def transition_function(s, a, next_s):\n",
    "    p = transition_probabilities[a, s, next_s]\n",
    "    #print(s, a, next_s, p)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_function(s, a, next_s, r):\n",
    "    if s == 0 and next_s == 1 and a == 0:\n",
    "        return +5\n",
    "    if s == 2 and next_s == 0 and a == 1:\n",
    "        return -1\n",
    "    return 0\n",
    "    \n",
    "all_rewards = np.array([-1, 0, +5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def terminate_function(s):\n",
    "    return s == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = MDPConfig(\n",
    "    state_space_type = SpaceType.DISCRETE,\n",
    "    action_space_type = SpaceType.DISCRETE,\n",
    "    transition_function_type = MDPTransitionType.SAS,\n",
    "    reward_function_type = MDPRewardType.SAS,\n",
    "    n_states = 3,\n",
    "    n_actions = 2,\n",
    ")\n",
    "\n",
    "model = MDP(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_range = Range(config.n_states)\n",
    "observation_wrapper = DiscreteObservationWrapper(model, state_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_range = Range(config.n_actions)\n",
    "action_wrapper = DiscreteActionWrapper(model, action_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.init(observation_wrapper, action_wrapper, transition_function, reward_function, terminate_function, all_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma = 0.9\n",
    "eps = 1e-3\n",
    "policy = ValueIterationDeterministicMDPPolicy(model, gamma, eps)\n",
    "policy.get_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy.fit()\n",
    "policy.get_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 => 2 0 True False {}\n"
     ]
    }
   ],
   "source": [
    "observation, info = model.reset(seed=42)\n",
    "\n",
    "for i in range(100):\n",
    "    action = model.action_space.sample()\n",
    "    observation, reward, terminated, truncated, info = model.step(action)\n",
    "    print(action, \"=>\", observation, reward, terminated, truncated, info)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 => 2 None False True {'Error': 'Probabilities sum to zero'}\n"
     ]
    }
   ],
   "source": [
    "observation, info = model.reset(seed=42)\n",
    "\n",
    "for i in range(100):\n",
    "    action = policy.get_policy()[observation]\n",
    "    observation, reward, terminated, truncated, info = model.step(action)\n",
    "    print(action, \"=>\", observation, reward, terminated, truncated, info)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MDP configuration\n",
    "mdp_config = MDPConfig(\n",
    "    state_space_type=SpaceType.DISCRETE,\n",
    "    action_space_type=SpaceType.DISCRETE,\n",
    "    transition_function_type=MDPTransitionType.SA_DETERMINISTIC,\n",
    "    reward_function_type=MDPRewardType.SA,\n",
    "    n_states=9,  # 3x3 grid\n",
    "    n_actions=4  # Up, Down, Left, Right\n",
    ")\n",
    "\n",
    "simple_gridworld_mdp = MDP(mdp_config)\n",
    "\n",
    "def transition_function(s, a, next_s):\n",
    "    if a == 0:  # Up\n",
    "        return max(s - 3, 0)\n",
    "    elif a == 1:  # Down\n",
    "        return min(s + 3, 8)\n",
    "    elif a == 2:  # Left\n",
    "        return max(s - 1, 0) if s % 3 != 0 else s\n",
    "    elif a == 3:  # Right\n",
    "        return min(s + 1, 8) if (s + 1) % 3 != 0 else s\n",
    "\n",
    "def reward_function(s, a, next_s, r):\n",
    "    if next_s == 8:  # Goal state\n",
    "        return 10\n",
    "    else:\n",
    "        return -10\n",
    "\n",
    "def terminate_function(s):\n",
    "    return s == 8\n",
    "\n",
    "simple_gridworld_mdp.init(\n",
    "    states=DiscreteObservationWrapper(simple_gridworld_mdp, Range(9)),\n",
    "    actions=DiscreteActionWrapper(simple_gridworld_mdp, Range(4)),\n",
    "    transition_function=transition_function,\n",
    "    reward_function=reward_function,\n",
    "    terminate_function=terminate_function\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = ValueIterationDeterministicMDPPolicy(simple_gridworld_mdp, gamma, eps)\n",
    "policy.get_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy.fit()\n",
    "policy.get_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
